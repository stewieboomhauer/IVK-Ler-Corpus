# Study 1.1.
	I use the correlation-based feature subset selection for machine learning approach to determine which of the complexity features are most informative for writing proficiency. By reducing the intercorrelation of the features within the subset, this method determines the subset of features that has the strongest correlation with the class to be predicted (in this thesis the language levels A0, A1 and A2).  I then report the most informative features for IVK-Ler corpus.
First, I want to determine which attribute in a given set of training feature vectors is most useful for discriminating between the classes to be learned. As classes I chose three target stories, 2, 7 and 12, as these were the stories that were repeated in three writing periods. These stories thus should correspond to three different language levels - A0: at the beginning of the experiment; A1.1: after 9 month from the beginning and A1.2. or A2: after 17 months from the beginning. With the help of these classes I want to determine the informativeness of each feature. For this, I calculated the information gain (IG) of each measure on the corpus using 10-folds cross-validation for training and testing. IG is a measure from information theory, which assesses the expected reduction in entropy for some class (the response variable) given a feature (the complexity measure for which the information gain is calculated). It was calculated using the Waikato Environment for Knowledge Analysis (WEKA) machine learning suit API, version 3.9 (Frank, Hall & Witten 2016; Hall et al. 2009). 
Due to poor results, the information gain algorithm was found to be irrelevant to the IVK-Ler data. The features that were analyzed as the most informative ones had a large scatter. This may be due to the fact that some samples contain several features with different values, while others lack these features. In this situation, the variation for each of these samples can be much greater than for samples with fewer different values. The fact that the features varied so much indicates the incompatibility of the data and the IG method. That is why I used another algorithm, which turned out to be fitter for the data, namely the One Rule (OneR) method for classification. 
After all features were rated, I eliminated any features that show a Pearson correlation smaller than Â±0.7 with each other.  I used RStudio for the correlation analysis. I estimate the maximum value of rank to determine which features can be regarded as informative and reliable for future analysis and to eliminate variation. I calculated the rank max by adding the rank delta (variation) to the rank to get the largest margin of error. The results of the application of OneR method will be presented in the next Section.
