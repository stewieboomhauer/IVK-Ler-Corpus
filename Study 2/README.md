# Study 2. 

Human correction of automatic annotations is crucial to corpus annotation. However, this necessary step is one of the most time-consuming parts of it. For this reason, it is helpful to use automatic tools that already provide the best possible results in order to reduce correction times and efforts. Even though many system evaluations achieve 97.85% accuracy results applying automatically POS-tagging , their results can hardly be applied reliably to L2 texts. 
Since the underlying Natural Language Processing (NLP) models are generally trained on native language data, the validity of the analysis cannot be taken for granted for learner language and can involve substantial conceptual challenges (Meurers & Dickinson 2017). It is thus important to determine: How much does learners’ language affect automated analyses like POS-tag?
In this study, I  do automatic POS-tagging by applying the two sequence labeling systems SpaCy and sticker on original (original hypothesis) and corrected (target hypothesis) learner texts  in order to find out how much learners’ language affects automated analyses like POS-tag. During the collection of this corpus, a human-annotated gold standard was created for the parts-of-speech of the original texts and target hypotheses. On the basis of this gold standard, I provide a comparison of SpaCy and sticker. There are no comparisons of such frameworks for L2 learner texts (with grammatical and orthographic errors) that I know of.  It is very useful information to know which sequence labeling tool to use for L2 learner texts to keep human annotation times at a minimum thanks to a good prior automatic annotation quality.


### Part-of-Speech Tagging

The annotation of the corpus followed a computer-assisted error analysis approach similar to the one described in Zinsmeister & Breckle (2010). Hence, it was done in two steps. The first step was an automatic annotation carried out with the help of the SpaCy29 framework and a less known sequence labeling framework called sticker30 (de Kok & Pütz, 2020). Both frameworks were used for parts-of-speech tagging for comparison purposes. Both tagging systems adhere to the universal dependency annotation scheme, which makes it easy to compare them. The annotation layer was added for both the original texts and the target hypotheses. The original sentences posed a particular challenge for the sequence labels because they contain a multitude of orthographic and grammatical errors. Even while many sequence labeling tasks can now be completed automatically and with great accuracy for correct texts, it is unclear if this also holds true for texts that contain errors. For this reason, as a second step, human annotation was performed on top of the automatic labeling. To accelerate the time-consuming manual annotation, this step was framed as a correction of the automatic results. It was necessary for all labeling tasks, because neural sequence labels are not perfect yet. Even for part-of-speech tagging, where most of the modern labeling systems constantly reach F1 scores of over 96%, the additional human correction phase had to be done.


